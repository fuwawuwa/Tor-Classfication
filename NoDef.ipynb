{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NoDef.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKHDxo0PjLNR"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzg1q6XRm0kh",
        "outputId": "854d1053-3f98-4d16-ffe2-be3d688401ca"
      },
      "source": [
        "%cd drive/MyDrive/TOR_classfication/\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/TOR_classfication\n",
            "Mon May 31 04:17:47 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw6uSqO7nCpE"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import wandb\n",
        "import copy\n",
        "import math\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhUtHLPHnJk5",
        "outputId": "91a4dc67-5dbf-4dde-86c4-6eec8395f552"
      },
      "source": [
        "!wandb login 794db4eff86a98642f46c66ffe2f8cdc98fd4e14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X70tV9pxnueP"
      },
      "source": [
        "class CausalConv1d(nn.Module):\n",
        "    \"\"\"\n",
        "    输入输出形状一致\n",
        "    扩张因果卷积\n",
        "    只适用 stride=1 or 2 其余情况等式可能会发生变化\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_size, out_size, kernel_size, stride=1, dilation=1):\n",
        "        \"\"\"\n",
        "        :param in_size: int 输入通道数\n",
        "        :param out_size: int 输出通道数\n",
        "        :param kernel_size: int 卷积核大小\n",
        "        :param stride: int 步幅\n",
        "        :param dilation: int 扩张率\n",
        "        \"\"\"\n",
        "        super(CausalConv1d, self).__init__()\n",
        "        self.pad = (kernel_size - 1) * dilation\n",
        "        self.bias = self.pad // stride\n",
        "        self.conv1 = nn.Conv1d(in_size, out_size, kernel_size, padding=self.pad, stride=stride, dilation=dilation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x[..., :-self.bias]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, use_1x1conv=False, strides=1):\n",
        "        \"\"\"\n",
        "        :param input_channels: int, 输入通道数\n",
        "        :param output_channels: int, 输出通道数\n",
        "        :param use_1x1conv: boolean 是否使用1x1卷积核\n",
        "        :param strides: int 第一个因果卷积块的stride 默认为1\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.conv1 = CausalConv1d(input_channels, output_channels, kernel_size=3, stride=strides, dilation=1)\n",
        "        self.conv2 = CausalConv1d(output_channels, output_channels, kernel_size=3, dilation=2)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.conv4 = CausalConv1d(output_channels, output_channels, kernel_size=3, dilation=4)\n",
        "        self.conv5 = CausalConv1d(output_channels, output_channels, kernel_size=3, dilation=8)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(output_channels, eps=1e-5)\n",
        "        self.bn2 = nn.BatchNorm1d(output_channels, eps=1e-5)\n",
        "        self.bn3 = nn.BatchNorm1d(output_channels, eps=1e-5)\n",
        "        self.bn4 = nn.BatchNorm1d(output_channels, eps=1e-5)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y1 = self.bn2(self.conv2(self.relu(self.bn1(self.conv1(X)))))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y1 = self.relu(Y1 + X)\n",
        "        Y = self.bn4(self.conv5(self.relu(self.bn3(self.conv4(Y1)))))\n",
        "        return self.relu(Y + Y1)\n",
        "\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.residual0 = nn.Sequential(nn.Conv1d(1, 64, kernel_size=9, padding=4, dilation=1, stride=2),\n",
        "                                       nn.BatchNorm1d(64, eps=1e-5), nn.ReLU(),\n",
        "                                       nn.MaxPool1d(kernel_size=5, stride=2, padding=2), nn.Dropout(p=0.1))\n",
        "        self.residual1 = nn.Sequential(Residual(input_channels=64, output_channels=64), nn.Dropout(p=0.1))\n",
        "        self.residual2 = nn.Sequential(Residual(input_channels=64, output_channels=128, use_1x1conv=True, strides=2),\n",
        "                                       nn.Dropout(p=0.1))\n",
        "        self.residual3 = nn.Sequential(Residual(input_channels=128, output_channels=256, use_1x1conv=True, strides=2),\n",
        "                                       nn.Dropout(p=0.1))\n",
        "        self.residual4 = nn.Sequential(Residual(input_channels=256, output_channels=512, use_1x1conv=True, strides=2),\n",
        "                                       nn.Dropout(p=0.1))\n",
        "        self.out = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten())\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = self.residual4(self.residual3(self.residual2(self.residual1(self.residual0(X)))))\n",
        "        return self.out(Y)\n",
        "\n",
        "\n",
        "class Combine_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tcn = TCN()\n",
        "        self.mlp = nn.Sequential(nn.Flatten(), nn.Linear(5, 32), nn.BatchNorm1d(32), nn.ReLU())\n",
        "        self.combine = nn.Sequential(nn.Linear(512 + 32, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
        "                                     nn.Dropout(p=0.5), nn.Linear(256, 95))\n",
        "\n",
        "    def forward(self, X):\n",
        "        Dir = X[..., :-5]\n",
        "        metadata = X[..., -5:]\n",
        "        Y = torch.cat((self.tcn(Dir), self.mlp(metadata)), dim=1)\n",
        "        return self.combine(Y)\n",
        "\n",
        "\n",
        "net = Combine_net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCgaDuR_nvrs"
      },
      "source": [
        "def train(net, train_iter, dev_iter, num_epochs, lr, save_name, device):\n",
        "    # 使用gpu训练模型\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv1d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    net.apply(init_weights)\n",
        "    net.to(device)\n",
        "    wandb.watch(net, log=\"all\")\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=1e-3)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=math.sqrt(0.1), patience=2,verbose=True,min_lr=1e-5)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    num_batchs = len(train_iter)\n",
        "    best_dev_acc = 0\n",
        "    count = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        metric = Accumulator(3)\n",
        "        net.train()\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            optimizer.zero_grad()\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
        "            if (i + 1) % (num_batchs // 5) == 0 or i == num_batchs - 1:\n",
        "                train_l = metric[0] / metric[2]\n",
        "                train_acc = metric[1] / metric[2]\n",
        "                wandb.log({\"Train Accuracy\": train_acc, \"Training Loss\": l, \"Epoch\": epoch + (i + 1) / num_batchs})\n",
        "        dev_l, dev_acc = evaluate_accuracy_gpu(net, loss, dev_iter)\n",
        "        scheduler.step(dev_acc)\n",
        "        wandb.log({\"Dev Accuracy\": dev_acc, \"Dev Loss\": dev_l, \"Epoch\": epoch + 1})\n",
        "        if dev_acc > best_dev_acc:\n",
        "            best_model = copy.deepcopy(net)\n",
        "            best_dev_acc=dev_acc\n",
        "            best_epoch = epoch + 1\n",
        "            count = 0\n",
        "        else:\n",
        "            count += 1\n",
        "            if count == 10:\n",
        "                print(\"early stop!\")\n",
        "                print(\"best model epoch:%d dev_acc:%.4f\" % (best_epoch, best_dev_acc))\n",
        "                torch.save(best_model.state_dict(), save_name)\n",
        "                #torch.save(best_model, save_name)\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzkmfAoGnx1_"
      },
      "source": [
        "def proc_x_metadata(in_data):\n",
        "    ans = torch.zeros(in_data.shape[0], 1, in_data.shape[2] + 5)\n",
        "    for i in range(in_data.shape[0]):\n",
        "        in_packet = in_data[i].eq(-1).sum()\n",
        "        out_packet = in_data[i].eq(1).sum()\n",
        "        total_packet = in_packet + out_packet\n",
        "        metadata = torch.tensor(\n",
        "            [total_packet, in_packet, out_packet, in_packet / total_packet, out_packet / total_packet],\n",
        "            dtype=torch.float32).reshape(1, -1)\n",
        "        ans[i] = torch.cat((in_data[i], metadata), dim=1)\n",
        "    return ans\n",
        "\n",
        "\n",
        "def LoadDataNoDefCW(batch_size):\n",
        "    print(\"Loading non-defended dataset for closed-world scenario\")\n",
        "    # Point to the directory storing data\n",
        "    dataset_dir = 'ClosedWorld/NoDef/'\n",
        "\n",
        "    # X represents a sequence of traffic directions\n",
        "    # y represents a sequence of corresponding label (website's label)\n",
        "\n",
        "    # Load training data\n",
        "    with open(dataset_dir + 'X_train_NoDef.pkl', 'rb') as handle:\n",
        "        X_train = torch.tensor(pickle.load(handle, encoding='bytes'), dtype=torch.float32)\n",
        "        X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "        X_train = proc_x_metadata(X_train)\n",
        "    with open(dataset_dir + 'y_train_NoDef.pkl', 'rb') as handle:\n",
        "        y_train = torch.tensor(pickle.load(handle, encoding='bytes'), dtype=torch.int64)\n",
        "\n",
        "    # Load validation data\n",
        "    with open(dataset_dir + 'X_valid_NoDef.pkl', 'rb') as handle:\n",
        "        X_valid = torch.tensor(pickle.load(handle, encoding='bytes'), dtype=torch.float32)\n",
        "        X_valid = X_valid.reshape(X_valid.shape[0], 1, X_valid.shape[1])\n",
        "        X_valid = proc_x_metadata(X_valid)\n",
        "    with open(dataset_dir + 'y_valid_NoDef.pkl', 'rb') as handle:\n",
        "        y_valid = torch.tensor(pickle.load(handle, encoding='bytes'), dtype=torch.int64)\n",
        "\n",
        "    print(\"Data dimensions:\")\n",
        "    print(\"X: Training data's shape : \", X_train.shape)\n",
        "    print(\"y: Training data's shape : \", y_train.shape)\n",
        "    print(\"X: Validation data's shape : \", X_valid.shape)\n",
        "    print(\"y: Validation data's shape : \", y_valid.shape)\n",
        "\n",
        "    # return X_train, y_train, X_valid, y_valid\n",
        "    return (DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True, num_workers=2),\n",
        "            DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=True, num_workers=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDUCwLFpn0ij"
      },
      "source": [
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "# 准确率计算 返回准确的个数\n",
        "def accuracy(y_hat, y):\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[0] > 1:\n",
        "        y_hat = y_hat.argmax(axis=1)\n",
        "    cmp = y_hat.type(y.dtype) == y\n",
        "    return float(cmp.type(y.dtype).sum())\n",
        "\n",
        "\n",
        "def evaluate_accuracy_gpu(net, loss, data_iter, device=None):\n",
        "    # 在gpu上评估模型精度\n",
        "    net.eval()\n",
        "    if not device:\n",
        "        device = next(iter(net.parameters())).device\n",
        "    metric = Accumulator(3)\n",
        "    for X, y in data_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat = net(X)\n",
        "        l = loss(y_hat, y)\n",
        "        metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "\n",
        "# Top k 预测 计算准确数  默认top1\n",
        "def top_k_accuracy(y_hat, y, k=1):\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[0] > 1:\n",
        "        values, indices = y_hat.topk(k, dim=1)\n",
        "    cmp = indices.type(y.dtype) == y.reshape(y.shape[0], 1)\n",
        "    return float(cmp.type(y.dtype).sum())\n",
        "\n",
        "\n",
        "# 默认top1\n",
        "def evaluate_top_k_accuracy_gpu(net, data_iter, device=None, k=1):\n",
        "    # 在gpu上评估模型精度\n",
        "    net.eval()\n",
        "    if not device:\n",
        "        device = next(iter(net.parameters())).device\n",
        "    metric = Accumulator(2)\n",
        "    for X, y in data_iter:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        metric.add(top_k_accuracy(net(X), y, k), y.numel())\n",
        "    return metric[0] / metric[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "0ClQQz98n16p",
        "outputId": "abb4f35f-d100-4d79-d5a9-8561d31c1dc7"
      },
      "source": [
        "wandb.init(project=\"NoDef\")\n",
        "config = wandb.config\n",
        "config.batch_size = 256\n",
        "config.epochs = 100\n",
        "config.lr = 0.001\n",
        "config.save_name = \"NoDef.pkl\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfuwafuwa\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.31<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">neat-water-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/fuwafuwa/NoDef\" target=\"_blank\">https://wandb.ai/fuwafuwa/NoDef</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/fuwafuwa/NoDef/runs/3bltmhv3\" target=\"_blank\">https://wandb.ai/fuwafuwa/NoDef/runs/3bltmhv3</a><br/>\n",
              "                Run data is saved locally in <code>/content/drive/My Drive/TOR_classfication/wandb/run-20210531_041822-3bltmhv3</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTHD9u9gn3Im",
        "outputId": "d1c648fd-b32a-4938-9012-2aa6ae553174"
      },
      "source": [
        "train_iter, dev_iter = LoadDataNoDefCW(config.batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading non-defended dataset for closed-world scenario\n",
            "Data dimensions:\n",
            "X: Training data's shape :  torch.Size([76000, 1, 5005])\n",
            "y: Training data's shape :  torch.Size([76000])\n",
            "X: Validation data's shape :  torch.Size([9500, 1, 5005])\n",
            "y: Validation data's shape :  torch.Size([9500])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCiVCuvZn4UB",
        "outputId": "8506b781-d861-49ec-9347-b876657e8fef"
      },
      "source": [
        "train(net, train_iter, dev_iter, config.epochs, config.lr, config.save_name, try_gpu())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    11: reducing learning rate of group 0 to 3.1623e-04.\n",
            "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch    23: reducing learning rate of group 0 to 3.1623e-05.\n",
            "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
            "early stop!\n",
            "best model epoch:26 dev_acc:0.9860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7NUf7fun5Sx",
        "outputId": "b2cf7284-652c-4427-d9f7-9676bf7a89a5"
      },
      "source": [
        "dataset_dir = 'ClosedWorld/NoDef/'\n",
        "# Load testing data\n",
        "with open(dataset_dir + 'X_test_NoDef.pkl', 'rb') as handle:\n",
        "    X_test = torch.tensor(pickle.load(handle, encoding='bytes'), dtype=torch.float32)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "    X_test = proc_x_metadata(X_test)\n",
        "with open(dataset_dir + 'y_test_NoDef.pkl', 'rb') as handle:\n",
        "    y_test = torch.tensor(pickle.load(handle, encoding='bytes'), dtype=torch.int64)\n",
        "print(\"Data dimensions:\")\n",
        "print(\"X: Testing data's shape : \", X_test.shape)\n",
        "print(\"y: Testing data's shape : \", y_test.shape)\n",
        "\n",
        "net=Combine_net()\n",
        "batch_size = 256\n",
        "save_name = \"NoDef.pkl\"\n",
        "test_iter = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "net.load_state_dict(torch.load(save_name))\n",
        "net.to(try_gpu())\n",
        "test_acc = evaluate_top_k_accuracy_gpu(net, test_iter, k=1)\n",
        "print(f'test acc {test_acc:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data dimensions:\n",
            "X: Testing data's shape :  torch.Size([9500, 1, 5005])\n",
            "y: Testing data's shape :  torch.Size([9500])\n",
            "test acc 0.9857\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}